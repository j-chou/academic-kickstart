<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | jedchou</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 19 Sep 2019 13:41:33 -0500</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>Increasing precision of offline tests - Part 1</title>
      <link>/post/2019-09-19-gee/</link>
      <pubDate>Thu, 19 Sep 2019 13:41:33 -0500</pubDate>
      <guid>/post/2019-09-19-gee/</guid>
      <description>

&lt;p&gt;One aspect of offline A/B tests that challenging than digital tests is that sample sizes are orders of magnitude smaller, so we really need to squeeze as much information as possible out of our data to get the answers we want. This often results in the use of more complex models compared to digital tests, where t-tests will usually suffice.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been reading the literature how to increase precision recently, and have come across to two primary strategies:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Stratified experiments (aka blocking)&lt;/li&gt;
&lt;li&gt;Repeated measurements + modeling&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this post I want to lay out how to implement these strategies for improving the precision of your experiments. There&amp;rsquo;s a lot of literature on these topics and a lot of opinions out there about what&amp;rsquo;s the best way to run experiments.&lt;/p&gt;

&lt;h2 id=&#34;blocking&#34;&gt;Blocking&lt;/h2&gt;

&lt;p&gt;Blocking is fairly straightforward. The basic idea is from sampling theory, where if you want to estimate a population quantity like the mean, rather than just taking a completely random sample, you sample from substrata where the outcomes are similar and then use a weighted average as the estimator. In an offline A/B test, we often have historical data about outcomes and can create strata of experimental units with similar pre-treatment outcomes. Then we randomly assign roughly half of the units in each strata to treatment and half to control. After the experiment is run and we have some post-treatment data, we would estimate treatment effects in each strata and then combine them via&lt;/p&gt;

&lt;p&gt;$$
\begin{eqnarray}
\hat{\tau} &amp;amp;=&amp;amp; \sum_{i=1}^{K} (\bar{Y}_{Ti} - \bar{Y}_{Ci}) \frac{N_{iT} + N_{iC}}{N}
\end{eqnarray}
$$&lt;/p&gt;

&lt;p&gt;where $\bar{Y}_{Ti}, \bar{Y}_{Ci}$ is the mean outcome for treatment and control units respectively in the $i^{th}$ strata, and $N_{Ci}$, $N_{Ti}$ are the number of treatment and control units respectively in the $i^{th}$ strata. The variance of this estimator is&lt;/p&gt;

&lt;p&gt;$$
\begin{eqnarray}
\hat{\sigma^2}(\hat{t}) &amp;amp;=&amp;amp; \sum_{i=1}^{K} \hat{\sigma^2}(\tau_i) (\frac{N_{iT} + N_{iC}}{N})^2
\end{eqnarray}
$$&lt;/p&gt;

&lt;p&gt;This often results in a substantial reduction in the variance of $\tau$.&lt;/p&gt;

&lt;p&gt;You could also use regression to incorporate information about these strata to increase precision of estimates, but that requires more care. David Freedman &lt;a href=&#34;https://www.stat.berkeley.edu/~census/neyregr.pdf&#34; target=&#34;_blank&#34;&gt;warned&lt;/a&gt; that &amp;ldquo;Regression adjustments are often made to experimental data. Since randomization does not justify the models, almost anything can happen.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;However, recently this &lt;a href=&#34;https://arxiv.org/pdf/1208.2301.pdf&#34; target=&#34;_blank&#34;&gt;paper&lt;/a&gt; by Winston Lin showed that some of the problems with using regression to analyze experimental data are taken care of when you have reasonably large sample size, the covariates are categorical, and you include all interactions between your treatment and your covariates.&lt;/p&gt;

&lt;h2 id=&#34;repeated-measurements&#34;&gt;Repeated measurements&lt;/h2&gt;

&lt;p&gt;Repeated measurements is not, as the name may suggest, just repeatedly measuring the same unit over time, keeping the treatments static. You actually want to randomize the treatment for each unit over time, so that each unit receives both test and control treatments. Then to estimate treatment effects we could use some model that takes into account correlation between measurements for the same unit, like a generalized estimating equation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# N : number of experimental units
# M : number of measurements, starting at time 0
# beta : true, homogeneous treatment effect
# initial_outcomes_0 : outcomes at time 0
# df : dataframe of size N x M of counterfactual outcomes under control
# df1 : df + beta; dataframe of size N x M of counterfactual outcomes under treatment

N = 1000
M = 50   
beta = 5
initial_outcomes_0 = np.random.normal(100, 10, size=N)
df = pd.DataFrame(np.zeros([N, M]))
df[0] = initial_outcomes_0

# generate random walks for each experimental unit
change = 10
for i in range(1, M):
    deltas = [-change if random.random() &amp;lt; 0.5 else change for i in range(N)]
    df[i] = df[i-1] + deltas
df1 = df + beta
df.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Below we visualize some rows in the dataframe df above. The outcomes start somewhere around 100 and undergo a random walk for an additional 49 steps.
The corresponding counterfactual time series under treatment for these time series is just the same time series shifted 5 units vertically.&lt;/p&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;/img/time_series_examples_20190924.png&#34; &gt;

&lt;img src=&#34;/img/time_series_examples_20190924.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Time series counterfactual outcomes under control for 10 random units.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;We&amp;rsquo;ll explore two experimental designs:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;(Reg) Randomly assign 500 units to control and 500 to treatment. Treatments are static across time for each unit.&lt;/li&gt;
&lt;li&gt;(RM) For each unit, randomly assign 25 time periods to treatment and 25 time periods to control.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We&amp;rsquo;ll also consider 3 ways to estimate treatment effects and standard errors.&lt;/p&gt;

&lt;h3 id=&#34;method-1-naive&#34;&gt;Method 1 - Naive&lt;/h3&gt;

&lt;p&gt;$$
\begin{eqnarray}
\hat{\tau} = \bar{Y_{T}} - \bar{Y_{C}}
\end{eqnarray}
$$&lt;/p&gt;

&lt;h3 id=&#34;method-2-cluster&#34;&gt;Method 2 - Cluster&lt;/h3&gt;

&lt;p&gt;$$
\begin{eqnarray}
\hat{\tau} = \bar{Y_{T}} - \bar{Y_{C}}
\end{eqnarray}
$$&lt;/p&gt;

&lt;h3 id=&#34;method-3-gee&#34;&gt;Method 3 - GEE&lt;/h3&gt;

&lt;p&gt;First let&amp;rsquo;s compare the two experimental designs. We&amp;rsquo;ll use the cluster-level estimator for average treatment effects since it&amp;rsquo;s the simplest. We simulated treatment assignments under the two experimental designs and &amp;hellip;&lt;/p&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;/img/time_series_examples_20190924.png&#34; &gt;

&lt;img src=&#34;/img/time_series_examples_20190924.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Time series counterfactual outcomes under control for 10 random units.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;The density plots are both centered at 5, the true treatment effect, showing that the cluster-level estimates of the ATE are unbiased under both experimental designs. However, repeated measurements experimental design is drastically more precise. The standard error for our estimates under the regular experimental design was around 12.2, while for the repeated measurements design it was 1.2, an almost 8x increase in precision. This pattern held for a wide range of random walk step sizes, from 1 to 100.&lt;/p&gt;

&lt;p&gt;Intuitively this is because outcomes are more similar within a unit than between units, so we can achieve a closer comparison by looking at outcomes at different time points for the same unit than comparing different units.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;What&amp;rsquo;s the right way to compute standard errors for our estimator under these two experimental designs?&lt;/em&gt; The most straightforward way is to just aggregate everything and use the typically t-test. Another way is to model the correlation between observations for the same unit with a generalized estimating equation. GEEs were introduced by Kung-Yee Liang and Scott Zeger in this landmark &lt;a href=&#34;https://academic.oup.com/biomet/article/73/1/13/246001&#34; target=&#34;_blank&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The GEE model is the following:
$$
\begin{eqnarray}
g(\mu_i) = x_i^{T} \beta + \epsilon_i
\end{eqnarray}
$$&lt;/p&gt;

&lt;p&gt;where $g$ is the link function, $\mu_{i}$ is the conditional mean of $y$ given $x$ (in this case $y$ is the outcome, and $x$ is the treatment) and $\beta$ is the vector of coefficients. In a GLM model, the covariance of $\epsilon_i$ would be diagonal, but the GEE model allows for nonzero elements outside the diagonal which means the $y_{i}$&amp;rsquo;s could be correlated.&lt;/p&gt;

&lt;p&gt;In Python, the GEE model is implemented in the statsmodels package and fit by:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import statsmodels.api as sm
import statsmodels.formula.api as smf

ind = sm.cov_struct.Exchangeable()
mod = smf.gee(&#39;value ~ treat&#39;, &#39;id&#39;, dat,
              cov_struct=ind,
              family=sm.families.Gaussian())
results = mod.fit()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The AR-1 covariance structure is actually more appropriate since our data are random walks, but an exchangeable model fits much faster and will suffice for this simulation.&lt;/p&gt;

&lt;p&gt;We got the following estimates of the ATE and standard errors:&lt;/p&gt;

&lt;p&gt;INSERT STANDARD errors&lt;/p&gt;

&lt;p&gt;which matches very well with the simulated estimates&amp;hellip;&lt;/p&gt;

&lt;h3 id=&#34;concluding-remarks&#34;&gt;Concluding Remarks&lt;/h3&gt;

&lt;p&gt;When possible, a repeated-measures experimental design can be very powerful, drastically reducing the required sample size. However, often treatments may have delayed effects on outcomes which&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Clusters and what to do about them</title>
      <link>/post/2019-09-01-clustered_standard_errors/</link>
      <pubDate>Tue, 06 Aug 2019 13:41:33 -0500</pubDate>
      <guid>/post/2019-09-01-clustered_standard_errors/</guid>
      <description>

&lt;h3 id=&#34;background&#34;&gt;Background&lt;/h3&gt;

&lt;p&gt;Pretty much all datasets have clusters. In A/B tests, clusters may be arbitrarily defined by the researcher, or they may actually constrain the treatment assignment.&lt;/p&gt;

&lt;p&gt;For example, in a store test one might only be able to assign treatments to geographic regions, so that all stores in the same region receive the same treatment. At the same time, we could also create arbitrary clusters of stores based on historical sales volume, square footage of the building, etc.&amp;ndash;variables which may not be used in the treatment assignment at all. In this post we&amp;rsquo;ll explore how to handle clusters when estimating treatment effects.&lt;/p&gt;

&lt;h3 id=&#34;pate-vs-cate&#34;&gt;PATE vs CATE&lt;/h3&gt;

&lt;p&gt;Part of the confusion surrounding how to analyze clustered data comes from a lack of clarity around the treatment effect that&amp;rsquo;s actually being estimated by the test statistic. There are often several quantities of interest in an experiment, including the most common Population Average Treatment Effect (PATE), the Cluster Average Treatment effect (CATE), and versions of these on the treated and untreated subgroups. Let&amp;rsquo;s start with an example to illustrate these differences.&lt;/p&gt;

&lt;p&gt;Suppose there are $K$ clusters of size $N_i$ with cluster-specific means $\mu_i$ for an outcome $y$ for $i = 1, &amp;hellip;, K$. The population mean is the weighted average&lt;/p&gt;

&lt;p&gt;$$
\mu_{p} = \sum_{i=1}^{K}\frac{N_i}{N}\mu_{i}
$$&lt;/p&gt;

&lt;p&gt;where $N = \sum_{i=1}^{K}N_i$ is the total population size. In general this is different from the cluster mean, which is the unweighted average&lt;/p&gt;

&lt;p&gt;$$
\mu_{c} = \frac{1}{K}\sum_{i=1}^{K}\mu_i
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Confidence Intervals - One Method to Rule Them All</title>
      <link>/post/2019-09-15-ci_one_method_to_rule_them_all/</link>
      <pubDate>Tue, 06 Aug 2019 13:41:33 -0500</pubDate>
      <guid>/post/2019-09-15-ci_one_method_to_rule_them_all/</guid>
      <description>&lt;p&gt;This is a short post on the delta method&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Double Machine Learning</title>
      <link>/post/2019-08-10-double_machine_learning/</link>
      <pubDate>Tue, 06 Aug 2019 13:41:33 -0500</pubDate>
      <guid>/post/2019-08-10-double_machine_learning/</guid>
      <description>&lt;p&gt;Questions:
1. What&lt;/p&gt;

&lt;p&gt;Double machine learning is a recent method from the econometrics literature that uses ML tools to get causal results. This method is good for situations where you have a low dimensional parameter of interest (typically a treatment effect), but a complex high-dimensional confounder. Formally, we have&lt;/p&gt;

&lt;p&gt;$$
\begin{eqnarray}
Y &amp;amp;=&amp;amp; D \theta + g(X) + \epsilon_{1}\\&lt;br /&gt;
D &amp;amp;=&amp;amp; m(X) + \epsilon_{2}
\end{eqnarray}
$$&lt;/p&gt;

&lt;p&gt;where $\mathbb{E}(\epsilon_1 | D, X) = 0$ and $\mathbb{E}(\epsilon_2 | X) = 0$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Instrumental Variables</title>
      <link>/post/2019-08-06-instrumental_variables/</link>
      <pubDate>Tue, 06 Aug 2019 13:41:33 -0500</pubDate>
      <guid>/post/2019-08-06-instrumental_variables/</guid>
      <description>

&lt;p&gt;Instrumental variables are a super useful tool in causal inference for getting around the problem of confounding.&lt;/p&gt;

&lt;p&gt;Suppose we&amp;rsquo;d like to estimate the average effect of a treatment $X$ like exposure to an ad on an outcome $Y$ like sales. For two weeks we track a group of people and whether they were exposed to the ad ($X = 1$) or not ($X = 0$) and what their sales were over the next month. We decide to fit a linear regression model to the data:
$$Y = \beta X + \epsilon$$
assuming $\mathbb{E}(\epsilon | X) = 0$.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Is $\beta$ the average treatment effect of $X$ on $Y$?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The answer is almost never, unless the data was collected from a carefully controlled A/B test. Basically this is because people in the treatment group ($X = 1$) are not the same as people in the control group ($X = 0$). People that saw the ad are more likely to buy than people who didn&amp;rsquo;t see the ad because of ad targeting. Our least-squares estimate $\hat{\beta}$ is likely biased upwards, and the better the ad targeting is for this ad, the worse the bias is.&lt;/p&gt;

&lt;p&gt;This problem is called &lt;em&gt;confounding&lt;/em&gt; and can be described mathematically by letting $W$ represent
confounding variables&lt;/p&gt;

&lt;p&gt;$$
\begin{eqnarray}
Y &amp;amp;=&amp;amp; \beta X + \gamma W + \epsilon \\&lt;br /&gt;
X &amp;amp;=&amp;amp; \alpha W + \nu
\end{eqnarray}
$$&lt;/p&gt;

&lt;p&gt;where $\mathbb{E}(\epsilon | X, W) = 0$ and $\mathbb{E}(\nu | W) = 0$. If $\mathbb{E}(\epsilon | X, W) = 0$, then &lt;a href=&#34;https://en.wikipedia.org/wiki/Ignorability&#34; target=&#34;_blank&#34;&gt;conditional ignorability&lt;/a&gt; is achieved, i.e. the potential outcomes $Y^{1}, Y^{0}$ are conditionally independent of $X$ given $W$. Then the least-squares estimate for $\beta$ in the $Y$-model above would be an unbiased estimate for the ATE.&lt;/p&gt;

&lt;p&gt;Now in theory, if you were for example Google, you could achieve conditional ignorability by taking all the variables $W$ you fed into your algorithms for ad targeting and model $Y = f(X, W)$ and get an unbiased estimate of the average treatment effect. There are several issues with this approach. First, you&amp;rsquo;re usually not Google and don&amp;rsquo;t have the data to adequately control for confounding. Second, even if you are Google, the algorithms used for ad targeting are enormously complex and rely on a large number of variables. What else can you do?&lt;/p&gt;

&lt;p&gt;In the happy case where you have an &lt;em&gt;instrumental variable&lt;/em&gt;, you can still get unbiased estimates of the ATE without even modeling the confounding. An &lt;em&gt;instrumental variable&lt;/em&gt; is just a variable that causes the treatment $X$ but doesn&amp;rsquo;t affect $Y$ except through $X$:&lt;/p&gt;

&lt;p&gt;$$
\begin{eqnarray}
Y &amp;amp;=&amp;amp; \beta X + \gamma W + \epsilon \\&lt;br /&gt;
X &amp;amp;=&amp;amp; \alpha W + \eta Z + \nu
\end{eqnarray}
$$&lt;/p&gt;

&lt;p&gt;To simplify the discussion, we assume linear relationships between all the variables. The diagram below illustrates the instrumental variable.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph TD;
  Z--&amp;gt;X;
  W--&amp;gt;X;
  W--&amp;gt;Y;
  X--&amp;gt;Y;

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To restate the issue, without $Z$, we cannot get unbiased estimates for $\beta$ in the model, because we don&amp;rsquo;t have good knowledge of $W$. The idea behind IV is that we can measure how a change in the instrument $Z$ induces a change in the outcome $Y$, independently of how $W$&lt;/p&gt;

&lt;p&gt;You can think about it like this: our target is the partial derivative of $Y$ with respect to $X$, $\beta = \partial Y / \partial X$. Let&amp;rsquo;s think about $\partial Y / \partial W$ for a moment. From the chain rule we have&lt;/p&gt;

&lt;p&gt;$$ \frac{\partial Y}{\partial Z} = \frac{\partial Y} {\partial X} \frac{\partial X}{\partial Z}$$&lt;/p&gt;

&lt;p&gt;so rearranging terms we have&lt;/p&gt;

&lt;p&gt;$$
\beta = \frac{\partial Y}{\partial X} &lt;br /&gt;
= \frac{ \frac{\partial Y}{\partial Z} }{ \frac{\partial X}{\partial Z} }
$$&lt;/p&gt;

&lt;p&gt;This leads to the Wald estimator for the ATE of a binary treatment variable:&lt;/p&gt;

&lt;p&gt;$$
\begin{eqnarray}
\hat{\beta}_{WALD} &amp;amp;=&amp;amp; \frac{\frac{\partial Y}{\partial Z}}{\frac{\partial X}{\partial Z}} = \frac{\mathbb{E}(Y|Z = 1) - \mathbb{E}(Y|Z = 0)}{\mathbb{E}(X|Z = 1) - \mathbb{E}(X|Z = 0)}
\end{eqnarray}
$$&lt;/p&gt;

&lt;p&gt;The idea is the same when the treatment is continuous: learn regression models $\mathbb{E}(Y | Z)$ and $\mathbb{E}(X | Z)$ and estimate the treatment effect as&lt;/p&gt;

&lt;p&gt;$$
\hat{\beta}_{2SLS} = \frac{\hat{\beta}_{Y \sim Z}}{\hat{\beta}_{X \sim Z}}
$$&lt;/p&gt;

&lt;p&gt;where $\hat{\beta}_{Y \sim Z}$ and $\hat{\beta}_{X \sim Z}$ are the least-squares estimates for the coefficients of $Z$ in the regression $Y \sim Z$ and $X \sim Z$, and 2SLS stands for two-stage least-squares. Below we run a simulation comparing the 2SLS estimator against a naive regression estimator, where the true average treatment effect is $\beta = 2.5$.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;n &amp;lt;- 1000
beta &amp;lt;- 2.5
w &amp;lt;- rnorm(n, -4, 1)
z &amp;lt;- rnorm(n, 2, 3)
x &amp;lt;- -8*w + 4*z + rnorm(n, 0, 1)
y &amp;lt;- beta*x + 5*w + rnorm(n, 0, 1)
df &amp;lt;- data.frame(&#39;w&#39; = w, &#39;z&#39; = z, &#39;x&#39; = x, &#39;y&#39; = y)

lr_mod &amp;lt;- lm(y ~ x, data = df)
lr_est &amp;lt;- lr_mod$coefficients[&#39;x&#39;]

yz_mod &amp;lt;- lm(y ~ z, data = df)
xz_mod &amp;lt;- lm(x ~ z, data = df)

iv_est &amp;lt;- yz_mod$coefficients[2]/xz_mod$coefficients[2]
iv_est # 2.51
lr_est # 2.32
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When we use a naive regression of $Y$ on $Z$ without accounting for the confounding $W$, our estimate is quite far off from the true effect of 2.5, but the 2SLS estimate is much closer to the truth.&lt;/p&gt;

&lt;h3 id=&#34;how-to-recognize-an-instrumental-variable&#34;&gt;How to recognize an instrumental variable?&lt;/h3&gt;

&lt;p&gt;A variable is a valid instrument if it causes the treatment but has no direct effect on the outcome. For example, the randomization of treatment assignment in an A/B test is a powerful instrument because it determines treatment exposure but has no direct effect on the outcome.&lt;/p&gt;

&lt;p&gt;Sometimes there are naturally occurring instruments as well. Distance to the nearest school is an instrument for the effect of college credits on future earnings because students that are further away from a college are likely to have less college credits, but otherwise distance to the nearest school shouldn&amp;rsquo;t have a direct effect future earnings.&lt;/p&gt;

&lt;p&gt;Even with this basic setup there are lots of interesting questions one could ask:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;How does the strength of the relationship between instrument and treatment affect the quality of IV estimates?&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Is it possible to test whether a variable is a valid instrument?&lt;/li&gt;
&lt;li&gt;What if relationships between variables are not linear?&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;What if I mistake an instrumental variable for a confounder?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These topics are beyond the scope of the this post, so we refer the interested reader to Morgan and Winship (2016). In summary, a strong instrumental variable makes the task of causal inference much more feasible. Without an IV, we would need to model confounding through careful variable selection and the use of flexible models.&lt;/p&gt;

&lt;!-- However, the question of identifiability--*with the variables I have, is it possible to figure out the causal effect of interest if I had infinite data on each of my variables?*--has mostly been laid to rest by the work of Judea Pearl and others, who invented the [back-door criterion](https://en.wikipedia.org/wiki/Confounding) and developed identification algorithms for casual diagrams. There&#39;s also a lot of current research on estimation of casual effects when there&#39;s no IV. See for example V. Chernozhukov et. al.&#39;s work on double machine learning and M. Oprescu et. al.&#39;s work on orthogonal random forests which we may discuss in future posts. --&gt;
</description>
    </item>
    
    <item>
      <title>Orthogonal Random Forests</title>
      <link>/post/2019-10-01-orthogonal_random_forests/</link>
      <pubDate>Tue, 06 Aug 2019 13:41:33 -0500</pubDate>
      <guid>/post/2019-10-01-orthogonal_random_forests/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The ABRACADABRA Problem</title>
      <link>/post/2019-09-01-abracadara/</link>
      <pubDate>Tue, 06 Aug 2019 13:41:33 -0500</pubDate>
      <guid>/post/2019-09-01-abracadara/</guid>
      <description>&lt;p&gt;Consider the following problems:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;How many times do you have to flip a fair coin on average to get 3 heads (HHH) in a row? to get (HHTT)?&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;If a monkey is sitting at a typewriter and hits each letter of the alphabet with equal probability &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;26&lt;/sub&gt;, how long on average will be take to type the sequence &amp;ldquo;ABRACADABRA&amp;rdquo;?&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There&amp;rsquo;s a really cool technique called Martingales that can be used to solve problems of this sort. A martingale is a sequence of random variables with the following property:&lt;/p&gt;

&lt;p&gt;$$
\mathbb{E}(X_n | X_1, &amp;hellip;, X_{n-1}) = \mathbb{E}(X_n | X_{n-1})
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing Omitted-Variables Bias</title>
      <link>/post/2019-08-25-bias_from_unmeasured_confounding/</link>
      <pubDate>Tue, 06 Aug 2019 13:41:33 -0500</pubDate>
      <guid>/post/2019-08-25-bias_from_unmeasured_confounding/</guid>
      <description>&lt;p&gt;I was thinking about the effects of unmeasured confounding on estimates of treatment effects and decided to run some simulations. I have the following setup:&lt;/p&gt;

&lt;p&gt;$$
\begin{eqnarray}
y &amp;amp;=&amp;amp; x + \alpha_1 w + \epsilon_1\\&lt;br /&gt;
x &amp;amp;=&amp;amp; \alpha_2 w + \epsilon_2
\end{eqnarray}
$$&lt;/p&gt;

&lt;p&gt;where $\mathbb{E}(y | x, w) \sim N(x + \alpha_1 w, 1)$ and $\mathbb{E}(x | w) \sim N(\alpha_2 w, 1)$. I varied the strength of the confounder $w$ by letting $1 \leq \alpha_1, \alpha_2 \leq 10$ and then fit the reduced model&lt;/p&gt;

&lt;p&gt;$$
y = \beta_{r} x
$$&lt;/p&gt;

&lt;p&gt;The graph below shows the bias $|\beta - \beta_{r}|$ in the estimate $\beta_r$ of the treatment effect as a function of $\alpha_1$ and $\alpha_2$.&lt;/p&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;/img/bias_linear_confounding_20190822.png&#34; &gt;

&lt;img src=&#34;/img/bias_linear_confounding_20190822.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Bias due to linear confounding.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;I was quite surprised by the highly nonlinear nature of the bias. It looks like for any $\alpha_1$, bias is the worst when $\alpha_2 = 1$.&lt;/p&gt;

&lt;!-- $$
f(\alpha_1, \alpha_2) = \\frac{\alpha_1}{2 \alpha_2 e^{1/\alpha_2} - 1}
$$ --&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;/img/bias_linear_confounding_alpha12_20190822.png&#34; &gt;

&lt;img src=&#34;/img/bias_linear_confounding_alpha12_20190822.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Bias due to linear confounding.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;I also made some visualizations of the bias under nonlinear confounding and the results were equally fascinating.&lt;/p&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;/img/bias_quadratic_confounding_alpha1_20190822.png&#34; &gt;

&lt;img src=&#34;/img/bias_quadratic_confounding_alpha1_20190822.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Bias due to quadratic confounding.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;These graphs motivated me to work out the bias analytically at least for the linear situation. For simplicity, let&amp;rsquo;s assume $Var(W) = Var(\epsilon_1) = Var(\epsilon_2) = 1$.
Then the least squares estimates for the coefficient of $X$ in the regression $Y \sim X$ is&lt;/p&gt;

&lt;p&gt;$$
\begin{eqnarray}
R_{yx} &amp;amp;=&amp;amp; \frac{1}{Var(X)}\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y}) \\&lt;br /&gt;
&amp;amp;=&amp;amp; \frac{1}{Var(X)}\sum_{i=1}^{n}(x_i - \bar{x})(x_i + \alpha_1 w_i + \epsilon_{1i} - \bar{x} - \alpha_1 \bar{w} + \bar{\epsilon_1}) \\&lt;br /&gt;
&amp;amp;=&amp;amp; 1 + \frac{\alpha_1 Cov(X, W)}{Var(X)} \\&lt;br /&gt;
&amp;amp;=&amp;amp; 1 + \frac{\alpha_1 \alpha_2}{\alpha_{2}^2 + 1}
\end{eqnarray}
$$&lt;/p&gt;

&lt;p&gt;The omitted-variables bias can be derived for more general cases in this way as a function of the covariance matrix of the endogenous variables, but appears to get quickly more complicated.
This was a fun exercise, but in general to estimate sensitivity of a treatment effect estimate with respect to violations of nonconfoundedness, it&amp;rsquo;s much faster to just run some simulations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hello World!</title>
      <link>/post/2019-08-06-hello-world/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/2019-08-06-hello-world/</guid>
      <description>&lt;p&gt;First post! I haven&amp;rsquo;t decided exactly what will exist here,
but in general you can expect to find articles on things I&amp;rsquo;ve
recently been thinking about, whether that&amp;rsquo;s statistics or math or
some interesting paper I recently read.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
