<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>jedchou</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>jedchou</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 06 Aug 2019 13:41:33 -0500</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>jedchou</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Clustered </title>
      <link>/post/2019-09-01-clustered_standard_errors/</link>
      <pubDate>Tue, 06 Aug 2019 13:41:33 -0500</pubDate>
      <guid>/post/2019-09-01-clustered_standard_errors/</guid>
      <description>&lt;p&gt;The question of when its necessary to adjust standard errors for clusters in an experiment has bothered me for a long time. My goal in this post is to achieve some resolution on this topic.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Double Machine Learning</title>
      <link>/post/2019-08-10-double_machine_learning/</link>
      <pubDate>Tue, 06 Aug 2019 13:41:33 -0500</pubDate>
      <guid>/post/2019-08-10-double_machine_learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Instrumental Variables</title>
      <link>/post/2019-08-06-instrumental_variables/</link>
      <pubDate>Tue, 06 Aug 2019 13:41:33 -0500</pubDate>
      <guid>/post/2019-08-06-instrumental_variables/</guid>
      <description>

&lt;p&gt;Instrumental variables are a super useful tool in causal inference for getting around the problem of confounding.&lt;/p&gt;

&lt;p&gt;Suppose we&amp;rsquo;d like to estimate the average effect of a treatment $X$ like exposure to an ad on an outcome $Y$ like sales. For two weeks we track a group of people and whether they were exposed to the ad ($X = 1$) or not (($X = 0$) and what their sales were over the next month. We decide to fit a linear regression model to the data:
$$Y = \beta X + \epsilon$$
assuming $\mathbb{E}(\epsilon | X) = 0$.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Is $\beta$ the average treatment effect of $X$ on $Y$?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The answer is almost never, unless the data was collected from a carefully controlled A/B test. Basically this is because people in the treatment group ($X = 1$) are not the same as people in the control group ($X = 0$). People that saw the ad are more likely to buy than people who didn&amp;rsquo;t see the ad because of ad targeting. Our least-squares estimate $\hat{\beta}$ is likely biased upwards, and the better the ad targeting is for this ad, the worse the bias is.&lt;/p&gt;

&lt;p&gt;This problem is called &lt;em&gt;confounding&lt;/em&gt; and can be described mathematically by letting $W$ represent
confounding variables&lt;/p&gt;

&lt;p&gt;$$
\begin{eqnarray}
Y &amp;amp;=&amp;amp; \beta X + \gamma W + \epsilon \\&lt;br /&gt;
X &amp;amp;=&amp;amp; \alpha W + \nu
\end{eqnarray}
$$&lt;/p&gt;

&lt;p&gt;where $\mathbb{E}(\epsilon | X, W) = 0$ and $\mathbb{E}(\nu | W) = 0$. If $\mathbb{E}(\epsilon | X, W) = 0$, then &lt;a href=&#34;https://en.wikipedia.org/wiki/Ignorability&#34; target=&#34;_blank&#34;&gt;conditional ignorability&lt;/a&gt; is achieved, i.e. the potential outcomes $Y^{1}, Y^{0}$ are conditionally independent of $X$ given $W$. Then the least-squares estimate for $\beta$ in the model above would be an unbiased estimate for the ATE.&lt;/p&gt;

&lt;p&gt;Now in theory, if you were for example Google, you could achieve conditional ignorability by taking all the variables $W$ you fed into your algorithms for ad targeting and model just $Y = \beta X + \gamma W + \epsilon$ and get an unbiased estimate of the average treatment effect. There are several issues with this approach. First, you&amp;rsquo;re usually not Google and don&amp;rsquo;t have the data to adequately control for confounding. Second, even if you are Google, the algorithms used for ad targeting are enormously complex and rely on a large number of variables that may not even be clearly tracked.  What else can you do?&lt;/p&gt;

&lt;p&gt;In the happy case where you have an &lt;em&gt;instrumental variable&lt;/em&gt;, you can still get unbiased estimates of the ATE without even modeling the confounding. An &lt;em&gt;instrumental variable&lt;/em&gt; is just a variable that causes the treatment $X$ but doesn&amp;rsquo;t affect $Y$ except through $X$:&lt;/p&gt;

&lt;p&gt;$$
\begin{eqnarray}
Y &amp;amp;=&amp;amp; \beta X + \gamma W + \epsilon \\&lt;br /&gt;
X &amp;amp;=&amp;amp; \alpha W + \eta Z + \nu
\end{eqnarray}
$$&lt;/p&gt;

&lt;p&gt;To simplify the discussion, we assume linear relationships between all the variables. The diagram below illustrates the instrumental variable.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph TD;
  Z--&amp;gt;X;
  W--&amp;gt;X;
  W--&amp;gt;Y;
  X--&amp;gt;Y;

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To restate the issue, without $Z$, we cannot get unbiased estimates for $\beta$ in the model, because we don&amp;rsquo;t have good knowledge of $W$. The idea behind IV is that we can measure how a change in the instrument $Z$ induces a change in the outcome $Y$, independently of how $W$&lt;/p&gt;

&lt;p&gt;You can think about it like this: our target is the partial derivative of $Y$ with respect to $X$, $\beta = \partial Y / \partial X$. Let&amp;rsquo;s think about $\partial Y / \partial W$ for a moment. From the chain rule we have&lt;/p&gt;

&lt;p&gt;$$ \frac{\partial Y}{\partial Z} = \frac{\partial Y} {\partial X} \frac{\partial X}{\partial Z}$$&lt;/p&gt;

&lt;p&gt;so rearranging terms we have&lt;/p&gt;

&lt;p&gt;$$
\beta = \frac{\partial Y}{\partial X} &lt;br /&gt;
= \frac{ \frac{\partial Y}{\partial Z} }{ \frac{\partial X}{\partial Z} }
$$&lt;/p&gt;

&lt;p&gt;This leads to the Wald estimator for the ATE of a binary treatment variable:&lt;/p&gt;

&lt;p&gt;$$
\begin{eqnarray}
\hat{\beta}_{WALD} &amp;amp;=&amp;amp; \frac{\frac{\partial Y}{\partial Z}}{\frac{\partial X}{\partial Z}} = \frac{\mathbb{E}(Y|Z = 1) - \mathbb{E}(Y|Z = 0)}{\mathbb{E}(X|Z = 1) - \mathbb{E}(X|Z = 0)}
\end{eqnarray}
$$&lt;/p&gt;

&lt;p&gt;The idea is the same when the treatment is continuous: learn regression models $\mathbb{E}(Y | Z)$ and $\mathbb{E}(X | Z)$ and estimate the treatment effect as&lt;/p&gt;

&lt;p&gt;$$
\hat{\beta}_{2SLS} = \frac{\hat{\beta}_{Y \sim Z}}{\hat{\beta}_{X \sim Z}}
$$&lt;/p&gt;

&lt;p&gt;where $\hat{\beta}_{Y \sim Z}$ and $\hat{\beta}_{X \sim Z}$ are the least-squares estimates for the coefficients of $Z$ in the regression $Y \sim Z$ and $X \sim Z$, and 2SLS stands for two-stage least-squares. Below we run a simulation comparing the 2SLS estimator against a naive regression estimator, where the true average treatment effect is $\beta = 2.5$.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;n &amp;lt;- 1000
beta &amp;lt;- 2.5
w &amp;lt;- rnorm(n, -4, 1)
z &amp;lt;- rnorm(n, 2, 3)
x &amp;lt;- -8*w + 4*z + rnorm(n, 0, 1)
y &amp;lt;- beta*x + 5*w + rnorm(n, 0, 1)
df &amp;lt;- data.frame(&#39;w&#39; = w, &#39;z&#39; = z, &#39;x&#39; = x, &#39;y&#39; = y)

lr_mod &amp;lt;- lm(y ~ x, data = df)
lr_est &amp;lt;- lr_mod$coefficients[&#39;x&#39;]

yz_mod &amp;lt;- lm(y ~ z, data = df)
xz_mod &amp;lt;- lm(x ~ z, data = df)

iv_est &amp;lt;- yz_mod$coefficients[2]/xz_mod$coefficients[2]
iv_est # 2.51
lr_est # 2.32
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When we use a naive regression of $Y$ on $Z$ without accounting for the confounding $W$, our estimate is quite far off from the true effect of 2.5, but the 2SLS estimate is much closer to the truth.&lt;/p&gt;

&lt;h3 id=&#34;how-to-recognize-an-instrumental-variable&#34;&gt;How to recognize an instrumental variable?&lt;/h3&gt;

&lt;p&gt;A variable is a valid instrument if it causes the treatment but has no direct effect on the outcome. For example, the randomization of treatment assignment in an A/B test is a powerful instrument because it determines treatment exposure but has no direct effect on the outcome.&lt;/p&gt;

&lt;p&gt;Sometimes there are naturally occurring instruments as well. Distance to the nearest school is an instrument for the effect of college credits on future earnings because students that are further away from a college are likely to have less college credits, but otherwise distance to the nearest school shouldn&amp;rsquo;t have a direct effect future earnings.&lt;/p&gt;

&lt;p&gt;Even with this basic setup there are lots of interesting questions one could ask:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;How does the strength of the relationship between instrument and treatment affect the quality of IV estimates?&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Is it possible to test whether a variable is a valid instrument?&lt;/li&gt;
&lt;li&gt;What if relationships between variables are not linear?&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;What if I mistake an instrumental variable for a confounder?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These topics are beyond the scope of the this post, so we refer the interested reader to Morgan and Winship (2016). In summary, a strong instrumental variable makes the task of causal inference much more feasible. Without an IV, we would need to model confounding through careful variable selection and the use of flexible models.&lt;/p&gt;

&lt;!-- However, the question of identifiability--*with the variables I have, is it possible to figure out the causal effect of interest if I had infinite data on each of my variables?*--has mostly been laid to rest by the work of Judea Pearl and others, who invented the [back-door criterion](https://en.wikipedia.org/wiki/Confounding) and developed identification algorithms for casual diagrams. There&#39;s also a lot of current research on estimation of casual effects when there&#39;s no IV. See for example V. Chernozhukov et. al.&#39;s work on double machine learning and M. Oprescu et. al.&#39;s work on orthogonal random forests which we may discuss in future posts. --&gt;
</description>
    </item>
    
    <item>
      <title>Orthogonal Random Forests</title>
      <link>/post/2019-10-01-orthogonal_random_forests/</link>
      <pubDate>Tue, 06 Aug 2019 13:41:33 -0500</pubDate>
      <guid>/post/2019-10-01-orthogonal_random_forests/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The ABRACADABRA Problem</title>
      <link>/post/2019-09-01-abracadara/</link>
      <pubDate>Tue, 06 Aug 2019 13:41:33 -0500</pubDate>
      <guid>/post/2019-09-01-abracadara/</guid>
      <description>&lt;p&gt;I was looking through some&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hello World!</title>
      <link>/post/2019-08-06-hello-world/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/2019-08-06-hello-world/</guid>
      <description>&lt;p&gt;First post! I haven&amp;rsquo;t decided exactly what will exist here,
but in general you can expect to find articles on things I&amp;rsquo;ve
recently been thinking about, whether that&amp;rsquo;s statistics or math or
some interesting paper I recently read.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
