+++
date = "2019-09-19T13:41:33-05:00"
draft = true
title = "Increasing precision of offline tests - Part 1"

+++

One aspect of offline A/B tests that challenging than digital tests is that sample sizes are orders of magnitude smaller, so we really need to squeeze as much information as possible out of our data to get the answers we want. This often results in the use of more complex models compared to digital tests, where t-tests will usually suffice.

I've been reading the literature how to increase precision recently, and have come across to two primary strategies:

1. Stratified experiments (aka blocking)
2. Repeated measurements + modeling  

In this post I want to lay out how to implement these strategies for improving the precision of your experiments. There's a lot of literature on these topics and a lot of opinions out there about what's the best way to run experiments.


## Blocking
Blocking is fairly straightforward. The basic idea is from sampling theory, where if you want to estimate a population quantity like the mean, rather than just taking a completely random sample, you sample from substrata where the outcomes are similar and then use a weighted average as the estimator. In an offline A/B test, we often have historical data about outcomes and can create strata of experimental units with similar pre-treatment outcomes. Then we randomly assign roughly half of the units in each strata to treatment and half to control. After the experiment is run and we have some post-treatment data, we would estimate treatment effects in each strata and then combine them via

$$
\begin{eqnarray}
\hat{\tau} &=& \sum\_{i=1}^{K} (\bar{Y}\_{Ti} - \bar{Y}\_{Ci}) \frac{N\_{iT} + N\_{iC}}{N}
\end{eqnarray}
$$

where $\bar{Y}\_{Ti}, \bar{Y}\_{Ci}$ is the mean outcome for treatment and control units respectively in the $i^{th}$ strata, and $N\_{Ci}$, $N_{Ti}$ are the number of treatment and control units respectively in the $i^{th}$ strata. The variance of this estimator is

$$
\begin{eqnarray}
\hat{\sigma^2}(\hat{t}) &=& \sum\_{i=1}^{K} \hat{\sigma^2}(\tau\_i) (\frac{N\_{iT} + N\_{iC}}{N})^2
\end{eqnarray}
$$

This often results in a substantial reduction in the variance of $\tau$.

You could also use regression to incorporate information about these strata to increase precision of estimates, but that requires more care. David Freedman [warned](https://www.stat.berkeley.edu/~census/neyregr.pdf) that "Regression adjustments are often made to experimental data. Since randomization does not justify the models, almost anything can happen."

However, recently this [paper](https://arxiv.org/pdf/1208.2301.pdf) by Winston Lin showed that some of the problems with using regression to analyze experimental data are taken care of when you have reasonably large sample size, the covariates are categorical, and you include all interactions between your treatment and your covariates.


## Repeated measurements
Repeated measurements is not, as the name may suggest, just repeatedly measuring the same unit over time, keeping the treatments static. You actually want to randomize the treatment for each unit over time, so that each unit receives both test and control treatments. Then to estimate treatment effects we could use some model that takes into account correlation between measurements for the same unit, like a generalized estimating equation.

```python
# N : number of experimental units
# M : number of measurements, starting at time 0
# beta : true, homogeneous treatment effect
# initial_outcomes_0 : outcomes at time 0
# df : dataframe of size N x M of counterfactual outcomes under control
# df1 : df + beta; dataframe of size N x M of counterfactual outcomes under treatment

N = 1000
M = 50   
beta = 5
initial_outcomes_0 = np.random.normal(100, 10, size=N)
df = pd.DataFrame(np.zeros([N, M]))
df[0] = initial_outcomes_0

# generate random walks for each experimental unit
change = 10
for i in range(1, M):
    deltas = [-change if random.random() < 0.5 else change for i in range(N)]
    df[i] = df[i-1] + deltas
df1 = df + beta
df.head()
```
Below we visualize some rows in the dataframe df above. The outcomes start somewhere around 100 and undergo a random walk for an additional 49 steps.
The corresponding counterfactual time series under treatment for these time series is just the same time series shifted 5 units vertically.

{{< figure library="true" src="time_series_examples_20190924.png" title="Time series counterfactual outcomes under control for 10 random units." lightbox="true" >}}

We'll explore two experimental designs:

1. (Reg) Randomly assign 500 units to control and 500 to treatment. Treatments are static across time for each unit.
2. (RM) For each unit, randomly assign 25 time periods to treatment and 25 time periods to control.

We'll also consider 3 ways to estimate treatment effects and standard errors.   

### Method 1 - Naive
$$
\begin{eqnarray}
\hat{\tau} = \bar{Y\_{T}} - \bar{Y\_{C}}
\end{eqnarray}
$$

### Method 2 - Cluster
$$
\begin{eqnarray}
\hat{\tau} = \bar{Y\_{T}} - \bar{Y\_{C}}
\end{eqnarray}
$$

### Method 3 - GEE

First let's compare the two experimental designs. We'll use the cluster-level estimator for average treatment effects since it's the simplest. We simulated treatment assignments under the two experimental designs and ...

{{< figure library="true" src="time_series_examples_20190924.png" title="Time series counterfactual outcomes under control for 10 random units." lightbox="true" >}}

The density plots are both centered at 5, the true treatment effect, showing that the cluster-level estimates of the ATE are unbiased under both experimental designs. However, repeated measurements experimental design is drastically more precise. The standard error for our estimates under the regular experimental design was around 12.2, while for the repeated measurements design it was 1.2, an almost 8x increase in precision. This pattern held for a wide range of random walk step sizes, from 1 to 100.

Intuitively this is because outcomes are more similar within a unit than between units, so we can achieve a closer comparison by looking at outcomes at different time points for the same unit than comparing different units.

*What's the right way to compute standard errors for our estimator under these two experimental designs?* The most straightforward way is to just aggregate everything and use the typically t-test. Another way is to model the correlation between observations for the same unit with a generalized estimating equation. GEEs were introduced by Kung-Yee Liang and Scott Zeger in this landmark [paper](https://academic.oup.com/biomet/article/73/1/13/246001).

The GEE model is the following:
$$
\begin{eqnarray}
g(\mu\_i) = x\_i^{T} \beta + \epsilon\_i
\end{eqnarray}
$$

where $g$ is the link function, $\mu_{i}$ is the conditional mean of $y$ given $x$ (in this case $y$ is the outcome, and $x$ is the treatment) and $\beta$ is the vector of coefficients. In a GLM model, the covariance of $\epsilon\_i$ would be diagonal, but the GEE model allows for nonzero elements outside the diagonal which means the $y\_{i}$'s could be correlated.

In Python, the GEE model is implemented in the statsmodels package and fit by:

```python
import statsmodels.api as sm
import statsmodels.formula.api as smf

ind = sm.cov_struct.Exchangeable()
mod = smf.gee('value ~ treat', 'id', dat,
              cov_struct=ind,
              family=sm.families.Gaussian())
results = mod.fit()
```

The AR-1 covariance structure is actually more appropriate since our data are random walks, but an exchangeable model fits much faster and will suffice for this simulation.

We got the following estimates of the ATE and standard errors:

INSERT STANDARD errors

which matches very well with the simulated estimates...

### Concluding Remarks
When possible, a repeated-measures experimental design can be very powerful, drastically reducing the required sample size. However, often treatments may have delayed effects on outcomes which
